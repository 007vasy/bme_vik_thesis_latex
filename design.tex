%----------------------------------------------------------------------------
\chapter{Design}
%----------------------------------------------------------------------------
\section{CAN fingerprint data set processing method}
%----------------------------------------------------------------------------
\subsection{Requirement specification}
%----------------------------------------------------------------------------
	\noindent
The final aim was to create data processing methods and calculated attributes from measured values for future prediction applications.
%----------------------------------------------------------------------------
	\subsubsection{Starting state}
The start state was a raw CAN bus physical sensory data in a *.mat format with different measurement frequencies by sensor types.
%----------------------------------------------------------------------------
	\subsubsection{Final state}
		\noindent
To achieve the final state, data sets require to transform to an CSV (Comma Separate Value) or RDS (R Data Structure) format for more convenient handling and calculating attributes.
		\noindent
The attributes are aggregations and summarization of connection between key values originating from expert knowledge, exploratory data analysis and several unsupervised learning technique.
%----------------------------------------------------------------------------
\subsubsection{Abstract Workflow}
To accomplish the final state from the starting state, the process was partitioned from on start-to-end into smaller steps. 

\begin{enumerate}
	\item {Cleaning:} To solve different measurement frequency problem by making a one-row/one-observation data frame.
	\item {Exploring:} Investigating the data with exploratory data analysis tools and unsupervised learning supported by the STILL workers's, engineers's and my acquaintances's expert knowledge.
 	\item {Specifying Attributes:} To introduce sustainable attribute calculating algorithms.
 	\item {Calculating RUL (Remaining useful life):} From received goal values (few tire measurement) and from the attributes calculated beforehand, present a RUL calculating algoritm for the MANTIS project.
\end{enumerate}

\begin{figure}[!ht]
\centering
\includegraphics[width=150mm, keepaspectratio]{figures/abstract_workflow.png}
\caption{Abstract workflow from raw data to final objective} 
\end{figure}

%----------------------------------------------------------------------------
\subsection{Expert knowledge and facts}
%----------------------------------------------------------------------------
\noindent
When some kind of industrial data set is examined and processed to make educated guess about the future principles and insights from expert knowledge are really helpful to make sense and understand the data in a higher sense.\footnote{In reality the vast majority of the insight came from exploratory data analysis and unsupervised learning}

\noindent
In summary, the main indicators of tire abrasion are between two tire measurement:
\begin{itemize}
	\item{traveled distance} 
	\item{elapsed time between}
	\item{elapsed time in high torque low speed state}
	\item{changing x direction in elapsed time}
	\item{changing y direction in elapsed time}
\end{itemize}

These aggregated values can be calculated from the raw sensor data, and from data with enough tire measurement point the RUL can be calculated with a help of a built statistical model.

%----------------------------------------------------------------------------
\subsection{Data processing workflow design}
%----------------------------------------------------------------------------
As it seems from the starting and final state, in first step the *mat files has to be transformed into usable data frame, and meanwhile the not useful columns has to be dropped to save precious processor time and data storage space, especially at handling the longer files. After that the attributes has to be calculated, then the remaining useful life.
To plan that using planning principles is necessary\footnote{Used bibliography \cite{CSDISTILLED}\cite{DATACAMP}\cite{LeanThinking}}.
	\subsubsection{Cleaning}
		\noindent
	As it has been described in the previous section, the *.mat files has to bee transformed to a data frame with only the useful columns remaining to the further calculations.
		\noindent
	To come up with a sustainable process, one has to iterate through more than one ideas, for the sake clarity, all ideas would be enumerated.
		\paragraph{Idea One}
			The first data shape transforming idea was to boxshort the data. In more detail:
\begin{enumerate}
	\item{examine the longest key value pair vector maximal timestamp} 
	\item{make a data frame with timestamp keys from zero to the max timestamp key got from the file in step of hundredth of a second}
	\item{search every value-s place with the help of the timestamp key}
	\item{interpolate for the missing values}
	\item{do it for all columns in the file}
	\item{do it for all files}
\end{enumerate}
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/cleaning_idea_one.png}
			\caption{First idea to clean the raw data} 
			\end{figure}
		\noindent
This is time consuming and slow even on the server.
		\paragraph{Idea Two}
To get rid of the speed problem, the second idea was to aggregate the values mean by second, this solution have to be more faster than the previous one, but if there time skip in the data\footnote{there is a lot of missing rows, sometimes minutes}
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/cleaning_idea_two.png}
			\caption{Second idea to read the raw data} 
			\end{figure}
		\paragraph{Idea Synthesis}
To get the advantage of the two methods it would be useful to merge them.
\begin{enumerate}
	\item{examine the longest key value pair vector maximal timestamp} 
	\item{make a data frame with timestamp keys from zero to the max timestamp key got from the file in step of a second}
	\item{aggregate the measurement average by second to second}
	\item{search every value-s place with the help of the timestamp key}
	\item{interpolate for the missing values}
	\item{do it for all columns in the file}
	\item{do it for all files}
\end{enumerate} 
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/cleaning_idea_synthesis.png}
			\caption{To optimize the cleaning workflow, merged the to processes together} 
			\end{figure}
\noindent
With this solution design, the high quality data frame, what can be used later is possible, and with this idea synthesis the problems with the previous ones are worked around.
%----------------------------------------------------------------------------
	\subsubsection{Exploring}
	explain why is it useful to do in a broader sense
	confirm expert knowledge
		\paragraph{Exploratory data analysis}
			explain why is it useful to do
			\subparagraph{Data summaries}
			enumerate data summary methods
			\subparagraph{Visualization}
			enumerate data visualization methods
		\paragraph{Unsupervised learning}
		explain what is it 
		enumerate some types
		confirm expert knowledge
%----------------------------------------------------------------------------
	\subsubsection{Specifying Attributes}
	from expert knowledge
	enumerate attributes
%----------------------------------------------------------------------------
	\subsubsection{Calculating RUL (Remaining useful life)}
	explain red flag in short
	calc what from what and why core principle and desired outcome framework abstract path to that
	calc hourly agg attributes
	interpolate tire abrosion
	avg the agg att/time = tire abrosion
	avg them in files
		\begin{figure}[!ht]
		\centering
		\includegraphics[width=150mm, keepaspectratio]{figures/CAN_fingerprint_RUL_calculation.png}
		\caption{Calculating RUL from cleaned data} 
		\end{figure}	
%----------------------------------------------------------------------------
%----------------------------------------------------------------------------
\section{Electrical fail prediction data set processing method}
%----------------------------------------------------------------------------
	\subsection{Requirement specification}
%----------------------------------------------------------------------------
From raw data and later described warning sequence from a random truck, the model have to estimate the remaining useful life from the exchange part ID possibilities.	
%----------------------------------------------------------------------------
	\subsection{Data processing workflow design}
%----------------------------------------------------------------------------
\cite{BALNWSCBOOK}
\cite{CSDISTILLED}
\cite{DATACAMP}
\cite{LeanThinking}
		\subsubsection{Cleaning}
		get rid of NA-s
		\begin{figure}[!ht]
		\centering
		\includegraphics[width=150mm, keepaspectratio]{figures/Electrical_fail_prediction_preprocessing_workflow.png}
		\caption{To optimize the cleaning workflow, merged the to processes together} 
		\end{figure}
		One have get rid of the resource wasting, fiddling values, and extract the can be useful columns. After that the graph building values can be selected.

		There is two group of the data:
		Nodes
		Edges

%----------------------------------------------------------------------------
		\subsubsection{Exploring}
		With visualization, make sense about the data 
		confirm expert knowledge
%----------------------------------------------------------------------------
		\subsubsection{Calculating RUL}
explain and reason the validity of the whole method
		explain red flag in short
		\begin{figure}[!ht]
		\centering
		\includegraphics[width=150mm, keepaspectratio]{figures/Calc_Fault_pred_RUL.png}
		\caption{To optimise the cleaning workflow, merged the to processes together} 
		\end{figure}
		Nodes: possible warnings and exchange part ID-s
		Edges truck ID, timestamp warning or exchange part ID
		form that we build a graph and with the given sequence we looking for the truck has similar path after that, from the last node of the sequence calculate possibility paths all of the exchange part ID-s. We display it with avg, min max mean time frame and calced path possibility.