%----------------------------------------------------------------------------
\chapter{Design}
%----------------------------------------------------------------------------
\section{CAN fingerprint dataset processing method}
%----------------------------------------------------------------------------
\subsection{Requirement specification}
%----------------------------------------------------------------------------
	\noindent
The final aim was to create data processing methods and calculated attributes from measured values for future prediction applications. Moreover, to come up with a sustainable prediction model, what is capable for prediction even when the amount of th provided target values are low.
%----------------------------------------------------------------------------
	\subsubsection{Initial state}
The initial state was a raw CAN bus physical sensory data in a *.mat format with different measurement frequencies by sensor types.
%----------------------------------------------------------------------------
	\subsubsection{Final state}
To achieve the final state, datasets require to be transformed into a CSV (Comma Separated Value) or RDS (R Data Structure) format for more convenient handling and calculating attributes.

The attributes are aggregations and summarizations of connection between key values originating from experts' knowledge, exploratory data analysis, and several unsupervised learning techniques. Along with several data transforming functions, to achieve the demanded Reaming Useful Life prediction.
\clearpage
%----------------------------------------------------------------------------
\subsubsection{Abstract Workflow}
To accomplish the final state from the starting state, the process was partitioned from on start-to-end into smaller steps. 

\begin{enumerate}
	\item {Cleaning:} to solve different measurement frequency problems by making a one-row/one-observation data frame.
	\item {Exploring:} to investigate the data with exploratory data analysis tools and unsupervised learning supported by the STILL workers', engineers' and my acquaintances' experts' knowledge.
 	\item {Specifying Attributes:} to introduce sustainable attribute calculating algorithms.
 	\item {Calculating RUL (Remaining Useful Life):} to develop a RUL calculation algorithm for the MANTIS project. This is based on received target values\footnote{few tire measurements}, and attributes calculated beforehand.
\end{enumerate}

\begin{figure}[!ht]
\centering
\includegraphics[width=150mm, keepaspectratio]{figures/abstract_workflow.png}
\caption{Abstract workflow from raw data, to final objective.} 
\end{figure}

%----------------------------------------------------------------------------
\subsection{Expert knowledge and facts}
%----------------------------------------------------------------------------
\paragraph\noindent
When some kind of industrial dataset is examined and processed to make educated guess about the future, principles and insights from experts' knowledge are actually helpful to interpret and understand the data in a higher sense\footnote{In reality the vast majority of the insight came from exploratory data analysis and unsupervised learning}. 

For example: There is four main forklift driving style.

In summary, the main indicators of tire abrasion between two tire measurements are:
\begin{itemize}
	\item{traveled distance,} 
	\item{elapsed time between,}
	\item{elapsed time in high torque low speed state,}
	\item{changing x direction in elapsed time,}
	\item{changing y direction in elapsed time.}
\end{itemize}

These aggregated values can be calculated from the raw sensor data, and with enough tire measurement points the RUL can be calculated with a help of a previously built statistical model.

%----------------------------------------------------------------------------
\subsection{Data processing workflow design}
%----------------------------------------------------------------------------
As it can be seen from the starting and final state, at first the *.mat files have to be transformed into usable data frame. Meanwhile, the not useful columns have to be dropped to save precious processor time and data storage space, especially by handling the longer files. After that the attributes should to be calculated, then the remaining useful life has to be estimated.
Various planning principles can be utilized during this design phase \cite{CSDISTILLED}, \cite{DATACAMP}, \cite{LeanThinking}.
%----------------------------------------------------------------------------
	\subsubsection{Cleaning}
	As it has been described in the previous section, the *.mat files have to be transformed to a data frame with only the useful columns remaining to further calculations.

	To establish a sustainable process, the iteration has to go through more than one ideas. For the sake of clarity, all ideas would be enumerated.
		\paragraph{Idea One:}
			The first data shape transforming idea was to box-short the data. Item by item:
\begin{enumerate}
	\item{to examine the longest key-value pair vector maximal timestamp,} 
	\item{to make a data frame with timestamp keys from zero to the maximum timestamp key got from the file in step of 10 milliseconds,}
	\item{to search every value's place with the help of timestamp key,}
	\item{to interpolate for missing values,}
	\item{to do it for all columns in the file,}
	\item{to do it for all files.}
\end{enumerate}
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/cleaning_idea_one.png}
			\caption{First idea (Idea One) to clean the raw data.} 
			\end{figure}
		\subparagraph\noindent
This is time consuming and slow even on the server, because it is not the optimal sorting algorithm for this kind of data representation \cite{CSDISTILLED}.
%----------------------------------------------------------------------------
		\paragraph{Idea Two:}
To get rid of the processing-speed problem, the second idea was to aggregate the values' mean by second. This solution has to be more faster than the previous one, but if there time skip in the data\footnote{there is a lot of missing rows, sometimes effecting minutes of operational data}, this idea would not be sufficient, either.
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/cleaning_idea_two.png}
			\caption{Second idea (Idea Two) to clean the raw data.} 
			\end{figure}
%----------------------------------------------------------------------------
		\paragraph{Idea Synthesis:}
To get the advantage of the two methods, it would be useful to merge the previous ideas.
\begin{enumerate}
	\item{examine the longest key-value pair vector maximal timestamp,} 
	\item{make a data frame with timestamp keys from zero to the maximal timestamp key got from the file in step of a second,}
	\item{aggregate the measurement average second by second,}
	\item{search every values' place with the help of timestamp key,}
	\item{interpolate to eliminate values,}
	\item{do it for all columns in the file,}
	\item{do it for all files.}
\end{enumerate} 
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/cleaning_idea_synthesis.png}
			\caption{To optimize the cleaning workflow, the to processes were merged together.} 
			\end{figure}
\subparagraph\noindent
With this solution design the high quality data frame, what can be used later, is possible, and with this idea synthesis the problems with the previous ones are worked around.
%----------------------------------------------------------------------------
	\subsubsection{Exploring}
	The data exploration and data examination is a required step for correct interpretation of the data and confirmation of facts coming from experts' knowledge. Without this "null hypothesis check" step, a lot of human labor and energy could be discarded.
%----------------------------------------------------------------------------
		\paragraph{Exploratory data analysis:}
			With these steps\footnote{data summaries and visualization} one can make himself ensure that dataset has connection with the real world, the values are close to reality, and further steps are worth a shot.
%----------------------------------------------------------------------------
			\subparagraph{Data summaries:}
			There are a lot of data summary methods\footnote{mean, median, SQRT, min, max and quarters}. With these aggregations ran on the dataset, the first impression is given. It is advisable to use these resource sparing methods.
%----------------------------------------------------------------------------			
			\subparagraph{Visualizations:}
			Human mind can effortlessly interpret large amount of data with correct visual representation. Furthermore, can check the connections and facts derived from experts' knowledge\footnote{Not to mention, amend that knowledge.}. 
%----------------------------------------------------------------------------		
		\paragraph{Unsupervised learning:}
		The "unsupervised learning" \cite{UnsuplearnBook} is a machine learning method used especially to unlabeled data. It is useful to find connections and correlation between the values in a higher perspective and confirm or question experts' knowledge.
%----------------------------------------------------------------------------
			\subparagraph{k-means clustering:}
			The k-means clustering \cite{k-means}, \cite{kmeans2} helps to estimate groups between observations with norming all values to one and calculating multi-dimensional distance between them.
			\subparagraph{Hierarchical clustering:}
			Hierarchical clustering \cite{h-clust}, \cite{h-clust2}, \cite{Clustering} is a k-means with a twist, because it clusters groups hierarchically, step by step with various distance calculation methods and top-down or bottom-up approach.
			\subparagraph{Principal component analysis:}
			The PCA \cite{PCA} algorithm makes an orthogonal, multi dimensional space from the values and places there the observations. This algorithm can tell the most significant values from the dataset.
%----------------------------------------------------------------------------
	\subsubsection{Specifying Attributes:}
	From experts' knowledge\footnote{and exploratory data analysis} the main components and indicators of the tire abrasion can be specified.

	These attributes are:
	\begin{itemize}
		\item{elapsed time,} 
		\item{traveled distance,} 
		\item{count in various speed and torque state,}
		\item{changing x direction,}
		\item{changing y direction,}
		\item{"is there weight" counter (max 3600 in a hour),}
		\item{steering wheel degree change derived by time aggregated by average\footnote{the torque and speed derived by time could be useful too}.}
	\end{itemize}
	All of them will be aggregated hour by hour\footnote{if the tire measurement interval goes down the aggregation window could and should too} from tire change or measurement.
%----------------------------------------------------------------------------
	\subsubsection{Calculating RUL}
	After the specified attributes are calculated, and aggregated and enough\footnote{more than one} tire measurement is available with accurate timestamp, the RUL can be calculated in the following way:

	\begin{enumerate}
		\item{to join tire measurement and hourly aggregated attributes by timestamp,}
		\item{to interpolate from tire measurements to hourly aggregations,}
		\item{to compute tire diameter change on all given measurement points,}
		\item{to calculate average diameter change by hourly aggregations,} 
		\item{to produce attributes hourly for a new measurement,}
		\item{to calculate tire diameter change for all attributes based on proportionality,}
		\item{to compute mean tire diameter change by hour,}
		\item{to summarize tire diameter change from last known diameter,}
		\item{if the summary is greater than a given constant, the tire should be changed,}
		\item{to compute reaming useful life on the last hours abrasion rate.}
	\end{enumerate}

	\paragraph\noindent
	The whole CAN fingerprint calculation workflow is in the Appendix \ref{appendix:CANFPRULCalc}.

	Other not implemented solutions:
	\begin{itemize}
		\item{calculate RUL with machine learning:} There were not enough data for this solution, the model will be over-fitted based on DataCamp case studies \cite{DataCampCaseStudies} 
		\item{calculate RUL with deep learning:} Similar to above.	
	\end{itemize}
	
%----------------------------------------------------------------------------
%----------------------------------------------------------------------------
\section{Electrical failure prediction dataset processing method}
%----------------------------------------------------------------------------
	\subsection{Requirement specification}
%----------------------------------------------------------------------------
This prediction model encompasses given CAN bus warning collection from STILL forklifts' electrical parts and SAP technical database containing technical workers' comments on each repair sessions. Moreover, it encloses occasionally exchanged parts' ID and name, and later described warning sequences from a random machine. The model has to estimate from these data the remaining useful life to a next repair session.

In the scope of this project iteration the exchange part IDs are the superior priority. The technical workers comments should be categorized with natural language processing method \cite{nlp}.
%----------------------------------------------------------------------------
	\subsection{Data processing workflow design}
%----------------------------------------------------------------------------
As it previously has been mentioned, the BAL Network science book \cite{BALNWSCBOOK} is an effective indicator of later specified solution. However, a lot of optimizing ideas come from the Computer Science Distilled book \cite{CSDISTILLED} and the usage of the Python language and the NetworkX package \cite{NetworkX} is the influence of the DataCamp e-learning platform \cite{DATACAMP}, and thinking in flows, process and value streams is caused by books such as the Lean Thinking book \cite{LeanThinking} and the Phoenix Project book \cite{PhoenixProject}.
%----------------------------------------------------------------------------
		\subsubsection{Conditions of the raw data}
		The dataset given for this subproject is consisting three files:
		\begin{itemize}
			\item{xtra\_All\_errors\_with\_metainfo.txt} size of 12.9 GB contains the warning logs of 789 forklifts, with timestamp, machine ID, the IDs of warnings, additional information which is out of the scope of this project\footnote{but in a further iteration could be necessary}, and a lot of NA columns.
			\item{ServiceReportSAPExtrakt\_FLMxFahrzeuge.txt} size of 224.7 MB contains an export from an SAP repair database with columns named for example: as "when was the machine repair session stared","when was the machine repair session ended", the machine's ID, the exchanged part ID number, some additional information\footnote{not all rows contain exchanged part ID. In these cases the technical worker's comments can be useful, but natural language processing is not the scope of this thesis.} and lot of NA columns.
			\item{Alle-SBs-ProActive\_droprows.xlsx} size of 4.5 MB nearly exactly the same columns like the one above, but form different forklifts and a little different column names.
		\end{itemize}
%----------------------------------------------------------------------------
		\subsubsection{Cleaning in two iterations}
		\begin{itemize}
			\item{Eliminating NA-s columns, and duplications:} This step is required to reduce the further resource cost of operations, and to have a filebase for further method development.
			\item{Discarding not useful columns:} For the first iteration graph building, the timestamp, machine ID, and ID of the warning is required from the first file mentioned before. From the second, and third file the repair sessions' starting and ending time the machine ID, and the exchange part ID is essential. When there is no exchange part ID just technical worker comment, for fast method's development, it could be handy to give a temporary distinct ID. The temporary ones have to differ from real exchange part IDs.\footnote{this can be a starting point of subcategorizing, with the help of NLP} 
		\end{itemize}
%----------------------------------------------------------------------------
		\subsubsection{Creating nodes' list and edges' list}
		From previously generated files the nodes and edges, the two vital components of the graph, can be generated. The nodes' list should contain all the distinct warning and exchange part ID-s, the edges' list should contain\footnote{One edge in a row.} the timestamps, the from and to nodes' ID , and the vehicles' ID.
%----------------------------------------------------------------------------
		\paragraph{Making the nodes' list:}
		The previously generated files contain a lot of duplicated warning and exchange part ID-s along with a multitudinous not needed columns.
		In this case the planned steps should be:
		\begin{enumerate}
			\item{to read in warning ID and exchange part ID columns from previously mentioned files,} 
			\item{to concatenate the two SAP originated lists,}
			\item{to drop duplications,}
			\item{to concatenate the lists,}
			\item{to save it.}
		 \end{enumerate}
%----------------------------------------------------------------------------
		\paragraph{Making the edges' list, machine by machine:}
		For easier handling, better performance, and disk space sparing reasons, it would be wise to generate the paths of all machines.
		The steps should be:
		\begin{enumerate}
			\item{reading in the largest file chunk by chunk,} 
			\item{grouping the information by machine ID,}
			\item{saving it to a corresponding machine's file,}
			\item{doing the previous 2 steps to the smaller files after reading it in as a whole,}
			\item{reading in the distinct machine paths one by one,}
			\item{ordering them by time,}
			\item{saving them in separated files.}
		 \end{enumerate}
		With these steps we get the distinct nodes and a lot of edges in different paths.

		The electrical failure prediction preprocessing workflow is in the Appendix \ref{appendix:FaultPredPreProc}
%----------------------------------------------------------------------------
		\subsubsection{Exploring:}
			Unlike the previous dataset, the cleaning steps are not as significant as they could be, because the data was given in a more efficient way. After the previously described NA and duplicated column dropping, the data exploring can begin. However, the end of data exploring is after the making of nodes and edges' list, because it is easy to check visually a correct graph representation\footnote{for example: the path is continuous or dashed}.
%----------------------------------------------------------------------------
			\subparagraph{Data summaries:}
			There are a lot of data summary methods (mean, median, SQRT, min, max and quarters). With these aggregations ran on the dataset, it is get a first impression. 
%----------------------------------------------------------------------------
			\subparagraph{Visualizations:}
			The human mind can easily interpret large amount of data with correct visual representation, and to go further, check the connections and facts derived from experts' knowledge\footnote{Not to mention, amend that knowledge.}.
%----------------------------------------------------------------------------
		\subsubsection{Calculating RUL}
%----------------------------------------------------------------------------
			\paragraph{Building the graph:}
			When the previous ordering, exploring, and fact checking are completed, the graph building can begin from generated files. Thus the base is the nodes' list, all nodes should be represented in the graph, when it comes to edges, the situation is different. For computational resource reasons not all of the paths should be included for sure. But the more the paths, the better is the prediction.
%----------------------------------------------------------------------------
\clearpage\paragraph{Computing steps:}
			If the graph has been built, the remaining useful life calculation steps should be the following:
			\begin{enumerate}
				\item{getting a warning sequence, what is not represented in the graph, from the same machine, in a chronological order,}
				\item{listing the machine' IDs between the oldest and the second oldest nodes given in the sequence}\footnote{All the nodes in the new sequence should be in the graph, as well.} 
				\item{iterating through the whole sequence and eliminating the machines which are absent from the current edges,}
				\item{eliminating the oldest node in the sequence and redo the previous search, when there is no machine shorted with the sequence,}
				\item{listing the shortest similar machine paths to the exchange part ID nodes, from the last and youngest node in the sequence.}
				\item{from the last node the possibility calculating to all exchange part ID nodes, based on the similar shortest machine path count to the specific exchange part ID nodes dividing by sum of to all the exchange part ID nodes,}
				\item{besides of possibility, computing the distance in time  to the specific exchange part ID node, to give more insights about the remaining useful life, based on the similar shortest machine's distance in time,}
				\item{the result should look with columns like: exchange part ID node, minimal, mean, median and maximal time to that node, and the computed possibility arranged by possibility and time,} 
		 		\item{the RUL is the highest possibility with the shortest remaining time.}
		 	\end{enumerate}
		
		\paragraph\noindent	
		The calculation of the whole table could be profitable because the technical workers can solve more problem at one repair sessions which could cause longer timespan on duty for the machines.

		The whole electrical failure prediction preprocessing workflow is in the Appendix \ref{appendix:FaultPredRULCalc}