%----------------------------------------------------------------------------
\chapter{Design}
%----------------------------------------------------------------------------
\section{CAN fingerprint dataset processing method}
%----------------------------------------------------------------------------
\subsection{Requirement specification}
%----------------------------------------------------------------------------
	\noindent
The final aim was to create data processing methods and calculated attributes from measured values for future prediction applications.
%----------------------------------------------------------------------------
	\subsubsection{Initial state}
The initial state was a raw CAN bus physical sensory data in a *.mat format with different measurement frequencies by sensor types.
%----------------------------------------------------------------------------
	\subsubsection{Final state}
To achieve the final state datasets require to transform to an CSV (Comma Separate Value) or RDS (R Data Structure) format for more convenient handling and calculating attributes.

The attributes are aggregations and summarization of connection between key values originating from expert knowledge, exploratory data analysis and several unsupervised learning technique.
%----------------------------------------------------------------------------
\subsubsection{Abstract Workflow}
To accomplish the final state from the starting state the process was partitioned from on start-to-end into smaller steps. 

\begin{enumerate}
	\item {Cleaning:} To solve different measurement frequency problem by making a one-row/one-observation data frame.
	\item {Exploring:} To investigate the data with exploratory data analysis tools and unsupervised learning supported by the STILL workers's, engineers's and my acquaintances's expert knowledge.
 	\item {Specifying Attributes:} To introduce sustainable attribute calculating algorithms.
 	\item {Calculating RUL (Remaining useful life):} To develop a RUL calculation algorithm for the MANTIS project. Based on received target values \footnote{few tire measurements} and from the attributes calculated beforehand.
\end{enumerate}

\begin{figure}[!ht]
\centering
\includegraphics[width=150mm, keepaspectratio]{figures/abstract_workflow.png}
\caption{Abstract workflow from raw data to final objective} 
\end{figure}

%----------------------------------------------------------------------------
\subsection{Expert knowledge and facts}
%----------------------------------------------------------------------------
\paragraph\noindent
When some kind of industrial dataset is examined and processed to make educated guess about the future, principles and insights from expert knowledge are actually helpful to interpret and understand the data in a higher sense.\footnote{In reality the vast majority of the insight came from exploratory data analysis and unsupervised learning}

In summary, the main indicators of tire abrasion are between two tire measurements:
\begin{itemize}
	\item{traveled distance} 
	\item{elapsed time between}
	\item{elapsed time in high torque low speed state}
	\item{changing x direction in elapsed time}
	\item{changing y direction in elapsed time}
\end{itemize}

These aggregated values can be calculated from the raw sensor data, and with enough tire measurement point the RUL can be calculated with a help of a previously built statistical model.

%----------------------------------------------------------------------------
\subsection{Data processing workflow design}
%----------------------------------------------------------------------------
As it seems from the starting and final state, at first the *mat files have to be transformed into usable data frame. Meanwhile the not useful columns have to be dropped to save precious processor time and data storage space, especially at handling the longer files. After that the attributes have to be calculated, then the remaining useful life.
To plan that using planning principles is necessary\footnote{Used bibliography \cite{CSDISTILLED}\cite{DATACAMP}\cite{LeanThinking}}.
%----------------------------------------------------------------------------
	\subsubsection{Cleaning}
	As it has been described in the previous section, the *.mat files have to be transformed to a data frame with only the useful columns remaining to the further calculations.

	To establish a sustainable process, the iteration has to go through more than one ideas, for the sake of clarity, all ideas would be enumerated.
		\paragraph{Idea One}
			The first data shape transforming idea was to boxshort the data. Item by item:
\begin{enumerate}
	\item{to examine the longest key value pair vector maximal time stamp} 
	\item{to makeg a data frame with time stamp keys from zero to the maximum time stamp key got from the file in step of a second's hundredth}
	\item{to search every value's place with the help of the time stamp key}
	\item{to interpolate for missing values}
	\item{to do it for all columns in the file}
	\item{to do it for all files}
\end{enumerate}
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/cleaning_idea_one.png}
			\caption{First idea to clean the raw data} 
			\end{figure}
		\subparagraph\noindent
This is time consuming and slow even on the server, because not the optimal sorting algorithm for this kind of data representation.\cite{CSDISTILLED}
		\paragraph{Idea Two}
To get rid of the speed problem the second idea was to aggregate the values mean by second. This solution has to be more faster than the previous one, but if there time skip in the data\footnote{there is a lot of missing rows, sometimes minutes} this idea also won't be sufficient.
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/cleaning_idea_two.png}
			\caption{Second idea to clean the raw data} 
			\end{figure}
		\paragraph{Idea Synthesis}
To get the advantage of the two methods it would be useful to merge the to previous  ideas.
\begin{enumerate}
	\item{examine the longest key value pair vector maximal time stamp} 
	\item{make a data frame with time stamp keys from zero to the maximal time stamp key got from the file in step of a second}
	\item{aggregate the measurement average second by second}
	\item{search every value-s place with the help of time stamp key}
	\item{interpolate for the missing values}
	\item{do it for all columns in the file}
	\item{do it for all files}
\end{enumerate} 
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/cleaning_idea_synthesis.png}
			\caption{To optimize the cleaning workflow, merged the to processes together} 
			\end{figure}
\subparagraph\noindent
With this solution design the high quality data frame, what can be used later, is possible and with this idea synthesis, the problems with the previous ones are worked around.
%----------------------------------------------------------------------------
	\subsubsection{Exploring}
	The data exploring and data examination is a required step for correctly interpret the data and confirm of facts coming from experts' knowledge. Without this "null hypothesis check" step lot of human labor and energy can be discarded.
		\paragraph{Exploratory data analysis}
			With this steps \footnote{data summaries and visualization} one can make himself ensure, the dataset has connection with the real world the values are close to reality and further step are worth a shot.
			\subparagraph{Data summaries}
			There are a lot of data summary methods \footnote{mean, median, SQRT, min, max and quarters}. With these aggregations ran on the dataset, the first impression is given. It's advisable to use these resource sparing methods.
			\subparagraph{Visualization}
			The human mind can effortlessly interpret large amount of data with correct visual representation, and to go further, can check the connections and facts derived from expert knowledge. \footnote{Not to mention, amend that knowledge.} 
		\paragraph{Unsupervised learning}\cite{UnsuplearnBook}
		The "unsupervised learning" is a machine learning method especially to unlabeled data it's useful to find connections and correlation between the values in a higher perspective and confirm or question expert knowledge.

		\subparagraph{k-means clustering}
		\cite{k-means}\cite{kmeans2} The k-means clustering helps to estimate groups between observations with norming all the values to one and calculating the multi-dimensional distance between them.
		\subparagraph{Hierarchical clustering}
		\cite{h-clust}\cite{h-clust2}\cite{Clustering} Hierarchical clustering s a k-means with a twist, because it clusters clusters hierarchically, step by step with various distance calculation method and top-down or bottom-up approach.
		\subparagraph{Principal component analysis}
		\cite{PCA}
		This algorithm makes an orthogonal, multi dimensional space from the values and places there the observations. This algorithm can tell the most significant values from the dataset.
%----------------------------------------------------------------------------
	\subsubsection{Specifying Attributes}
	From expert knowledge \footnote{and exploratory data analysis} the main components and indicators of the tire abrasion can be specified.

	These attributes are:
	\begin{itemize}
		\item{elapsed time} 
		\item{traveled distance} 
		\item{count in various speed and torque state}
		\item{changing x direction}
		\item{changing y direction}
		\item{is there weight counter (max 3600 in a hour)}
		\item{steering wheel degree change derived by time aggregated by average}\footnote{the torque and speed derived by time could be useful too}
	\end{itemize}
	All of them will be aggregated hour by hour\footnote{when the tire measurement interval goes down the aggregation window could and should too} from tire change or measurement.
%----------------------------------------------------------------------------
	\subsubsection{Calculating RUL (Remaining useful life)}
	After the specified attributes are calculated and aggregated and enough \footnote{more than one} tire measurement is available with accurate time stamp, the RUL can be calculated in the following way:

	\begin{enumerate}
		\item{to join tire measurement and hourly aggregated attributes by time stamp}
		\item{to interpolate from tire measurements to hourly aggregations}
		\item{to compute tire diameter change on all given measurement points}
		\item{to calculate average diameter change by hourly aggregations} with that tire diameter change by hourly attribute change is given
		\item{to produce attributes hourly for a new measurement}
		\item{to calculate tire diameter change for all attributes based on proportionality}
		\item{to compute the mean tire diameter change by hour}
		\item{to summarize the tire diameter change from last know diameter}
		\item{if the summary is greater than a given constant, the tire should be changed}
		\item{to compute reaming useful life on the last hours abrasion rate}
	\end{enumerate}
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=150mm, keepaspectratio]{figures/CAN_fingerprint_RUL_calculation.png}
		\caption{Calculating RUL from cleaned data} 
	\end{figure}
		\paragraph\noindent
	Other not implemented solutions:
	\begin{itemize}
		\item{calculate RUL with machine learning:} There was not enough data for this solution, the model will be over-fitted based on DataCamp case studies \cite{DataCampCaseStudies} 
		\item{calculate RUL with deep learning:} Similar to above.	
	\end{itemize}
	
%----------------------------------------------------------------------------
%----------------------------------------------------------------------------
\section{Electrical failure prediction dataset processing method}
%----------------------------------------------------------------------------
	\subsection{Requirement specification}
%----------------------------------------------------------------------------
\paragraph\noindent
This prediction model encompasses given CAn bus warning collection from STILL forklifts' electrical parts and SAP technical database containing technical workers' comments on each repair sessions. Moreover, it encompasses occasionally exchanged parts' ID and name, and later described warning sequences from a random machine. The model has to estimate from these data the remaining useful life to a next repair session.

\paragraph\noindent
In the scope of this project iteration the exchange part IDs are the superior priority the technical workers comments should be categorized with natural language processing method. \cite{nlp}
%----------------------------------------------------------------------------
	\subsection{Data processing workflow design}
%----------------------------------------------------------------------------
\paragraph\noindent
As it previously has been mentioned, the BAL Network science book \cite{BALNWSCBOOK} is an effective indicator of later specified solution. However, a lot of optimizing ideas come from the Computer Science Distilled book\cite{CSDISTILLED} the usage of the Python language and the NetworkX package\cite{NetworkX} is influence of the DataCamp e-learning platform \cite{DATACAMP}, and thinking in flows, process and value streams s caused by books such as the Lean Thinking book\cite{LeanThinking} and the Phoenix Project book \cite{PhoenixProject}


%----------------------------------------------------------------------------
		\subsubsection{Raw data's condition}
		The dataset given for this subproject is consisting three files:
		\begin{itemize}
			\item{xtra\_All\_errors\_with\_metainfo.txt} size of 12.9 GB contains the warning log of 789 forklifts, with time stamp, machine ID, the IDs of warnings, and additional information which is out of the scope of this project\footnote{but in a further iteration could be necessary}, and a lot of NA columns.
			\item{ServiceReportSAPExtrakt\_FLMxFahrzeuge.txt} size of 224.7 MB contains an export from an SAP repair database with columns named for example as "when was the machine repair session stared","when was the machine repair session ended", the machine's ID, the exchanged part ID number, some additional information \footnote{not all rows contain exchanged part ID, in these cases the technical worker's comments can be useful, but natural language processing is not the scope of this thesis.} and lot of NA columns.
			\item{Alle-SBs-ProActive\_droprows.xlsx} size of 4.5 MB nearly exactly the same columns like one above, but form different forklifts and a bit different column names.
		\end{itemize}
		\subsubsection{Cleaning in two iteration}
		\begin{itemize}
			\item{Eliminating NA-s columns, and duplications:} This step is required to reduce the further resource cost of the operations, and to have a filebase for further method development.
			\item{Discarding not useful columns} For the first iteration graph building, the time stamp, machine ID and ID of the warning is required from the firstly mentioned file. From the second and third repair session' starting and ending time the machine ID and the exchange part ID is essential. When there is no exchange part ID just technical worker comment, for fast method's development, it could be handy to give a temporary distinct ID. The temporary ones have to differ from real exchange part IDs. \footnote{this can be a starting point of subcategorizing, with the help of NLP} 
		\end{itemize}
		\subsubsection{Creating nodes list and edges list}
		\paragraph\noindent
		From previously generated files the nodes and edges, the two vital components of the graph can be generated. The nodes list should contains all the distinct warning and exchange part ID-s, the edges list should contain the time stamp, the from and to node's ID , and the vehicle ID.
		\paragraph{Making the nodes list}
		The previously generated files contain a lot of duplicated warning and exchange part ID along with a multitudinous not needed columns.
		Is this case the planned steps should be:
		\begin{enumerate}
			\item{to read in the warning ID and exchange part ID columns from previously mentioned files} 
			\item{to concatenate the two SAP originated lists}
			\item{to drop duplications}
			\item{to concatenate the lists}
			\item{to save it}
		 \end{enumerate}
		\paragraph{Making the edges list, machine by machine}
		For easier handling, better performance, and disk space reasons, would be wise to generate the path of all machines.
		The steps should be:
		\begin{enumerate}
			\item{reading in the largest file chunk by chunk} 
			\item{grouping the information by machine ID}
			\item{saving it to a corresponding machine's file}
			\item{doing the previous 2 steps to the smaller files after reading it in as a whole}
			\item{reading in the distinct machine paths one by one}
			\item{ordering them by time}
			\item{saving them in separated files}
		 \end{enumerate}
		With this steps we get the distinct nodes and a lot of edges in different paths. 
		\begin{figure}[!ht]
		\centering
		\includegraphics[width=150mm, keepaspectratio]{figures/Electrical_fail_prediction_preprocessing_workflow.png}
		\caption{Cleaning and ordering step by step} 
		\end{figure}
%----------------------------------------------------------------------------
		\subsubsection{Exploring}
			\paragraph\noindent
			Unlike the previous dataset, the cleaning steps aren't as significant as it could be, because the data was given in a more efficient way. After the later described NA and duplicated column dropping, the data exploring can begin. However, the end of data exploring is after the making of nodes and edges list, because it's easy to check visually a correct graph representation \footnote{for example: the path is continuous or dashed}.
			\subparagraph{Data summaries}
			There are a lot of data summary methods (mean, median, SQRT, min, max and quarters). With these aggregations ran on the dataset, it's get a first impression. 
			\subparagraph{Visualization}
			The human mind can easy interpret large amount of data with correct visual representation and to go further checked the connections and facts derived from expert knowledge. \footnote{Not to mention, amend that knowledge.}
%----------------------------------------------------------------------------
		\subsubsection{Calculating RUL}
			\paragraph{Building the graph}
			When the previous ordering and exploring and fact checking is completed, the graph building can begin from generated files. Thus, the base is the nodes list, moreover all nodes should be represented in the graph when it comes to edges the situation is different. For computational resource reasons, not all of the paths should be included for sure. But the more the path the better be the prediction.
			\paragraph{Computing steps:}
			If the graph has been build, the remaining useful life calculation steps should be the following:
			\begin{enumerate}
				\item{getting a warning sequence, what is not represented in the graph, from the same machine, in a chronological order}
				\item{the machine' IDs should be listed between the oldest and the second oldest nodes given in the sequence} \footnote{all the nodes in the new sequence should be int th graph too} 
				\item{iterating through the whole sequence and eliminating the machines which are absent in the current edges}
				\item{when there is no machine shorted the sequence with, eliminate the oldest node in the sequence and redo the previous search}
				\item{the list of machines with similar path are given, from the last and youngest node in the sequence. The shortest similar machine path should be listed to the exchange part ID nodes}
				\item{from the last node the possibility could be calculated to all exchange part ID nodes, based on the similar shortest machine path count to the specific exchange part ID nodes divided by sum of to all the exchange part ID nodes}
				\item{besides of possibility the time to the specific exchange part ID node could be computed to give more insights about the remaining useful life, based on the similar shortest machine's distance in time}
				\item{the result should look with columns like: exchange part ID node, minimal, mean, median and maximal time to that node, and the computed possibility arranged by possibility and time} 
		 		\item{the RUL is the highest possibility with the shortest remaining time}
		 	\end{enumerate}
		
		\paragraph\noindent	
		The calculation of the whole table could be profitable, because the technical workers can solve more problem at one repair sessions, this could cause longer timespan on duty.

		\#TODO explain red flags in short
		\begin{figure}[!ht]
		\centering
		\includegraphics[width=150mm, keepaspectratio]{figures/Calc_Fault_pred_RUL.png}
		\caption{To optimize the cleaning workflow, merged the to processes together} 
		\end{figure}