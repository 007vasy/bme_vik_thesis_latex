%----------------------------------------------------------------------------
\chapter{Design}
%----------------------------------------------------------------------------
\section{CAN fingerprint data set processing method}
%----------------------------------------------------------------------------
\subsection{Requirement specification}
%----------------------------------------------------------------------------
	\noindent
The final aim was to create data processing methods and calculated attributes from measured values for future prediction applications.
%----------------------------------------------------------------------------
	\subsubsection{Starting state}
The start state was a raw CAN bus physical sensory data in a *.mat format with different measurement frequencies by sensor types.
%----------------------------------------------------------------------------
	\subsubsection{Final state}
		\paragraph\noindent
To achieve the final state, data sets require to transform to an CSV (Comma Separate Value) or RDS (R Data Structure) format for more convenient handling and calculating attributes.
		\paragraph\noindent
The attributes are aggregations and summarization of connection between key values originating from expert knowledge, exploratory data analysis and several unsupervised learning technique.
%----------------------------------------------------------------------------
\subsubsection{Abstract Workflow}
To accomplish the final state from the starting state, the process was partitioned from on start-to-end into smaller steps. 

\begin{enumerate}
	\item {Cleaning:} To solve different measurement frequency problem by making a one-row/one-observation data frame.
	\item {Exploring:} Investigating the data with exploratory data analysis tools and unsupervised learning supported by the STILL workers's, engineers's and my acquaintances's expert knowledge.
 	\item {Specifying Attributes:} To introduce sustainable attribute calculating algorithms.
 	\item {Calculating RUL (Remaining useful life):} From received target values (few tire measurement) and from the attributes calculated beforehand, present a RUL calculating algorithm for the MANTIS project.
\end{enumerate}

\begin{figure}[!ht]
\centering
\includegraphics[width=150mm, keepaspectratio]{figures/abstract_workflow.png}
\caption{Abstract workflow from raw data to final objective} 
\end{figure}

%----------------------------------------------------------------------------
\subsection{Expert knowledge and facts}
%----------------------------------------------------------------------------
\paragraph\noindent
When some kind of industrial data set is examined and processed to make educated guess about the future principles and insights from expert knowledge are really helpful to make sense and understand the data in a higher sense.\footnote{In reality the vast majority of the insight came from exploratory data analysis and unsupervised learning}

\paragraph\noindent
In summary, the main indicators of tire abrasion are between two tire measurement:
\begin{itemize}
	\item{traveled distance} 
	\item{elapsed time between}
	\item{elapsed time in high torque low speed state}
	\item{changing x direction in elapsed time}
	\item{changing y direction in elapsed time}
\end{itemize}

These aggregated values can be calculated from the raw sensor data, and from data with enough tire measurement point the RUL can be calculated with a help of a built statistical model.

%----------------------------------------------------------------------------
\subsection{Data processing workflow design}
%----------------------------------------------------------------------------
As it seems from the starting and final state, in first step the *mat files has to be transformed into usable data frame, and meanwhile the not useful columns has to be dropped to save precious processor time and data storage space, especially at handling the longer files. After that the attributes has to be calculated, then the remaining useful life.
To plan that using planning principles is necessary\footnote{Used bibliography \cite{CSDISTILLED}\cite{DATACAMP}\cite{LeanThinking}}.
	\subsubsection{Cleaning}
		\paragraph\noindent
	As it has been described in the previous section, the *.mat files has to bee transformed to a data frame with only the useful columns remaining to the further calculations.
		\paragraph\noindent
	To come up with a sustainable process, one has to iterate through more than one ideas, for the sake clarity, all ideas would be enumerated.
		\paragraph{Idea One}
			The first data shape transforming idea was to boxshort the data. In more detail:
\begin{enumerate}
	\item{examine the longest key value pair vector maximal timestamp} 
	\item{make a data frame with timestamp keys from zero to the max timestamp key got from the file in step of hundredth of a second}
	\item{search every value-s place with the help of the timestamp key}
	\item{interpolate for the missing values}
	\item{do it for all columns in the file}
	\item{do it for all files}
\end{enumerate}
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/cleaning_idea_one.png}
			\caption{First idea to clean the raw data} 
			\end{figure}
		\subparagraph\noindent
This is time consuming and slow even on the server.
		\paragraph{Idea Two}
To get rid of the speed problem, the second idea was to aggregate the values mean by second, this solution have to be more faster than the previous one, but if there time skip in the data\footnote{there is a lot of missing rows, sometimes minutes}
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/cleaning_idea_two.png}
			\caption{Second idea to read the raw data} 
			\end{figure}
		\paragraph{Idea Synthesis}
To get the advantage of the two methods it would be useful to merge them.
\begin{enumerate}
	\item{examine the longest key value pair vector maximal timestamp} 
	\item{make a data frame with timestamp keys from zero to the max timestamp key got from the file in step of a second}
	\item{aggregate the measurement average by second to second}
	\item{search every value-s place with the help of the timestamp key}
	\item{interpolate for the missing values}
	\item{do it for all columns in the file}
	\item{do it for all files}
\end{enumerate} 
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/cleaning_idea_synthesis.png}
			\caption{To optimize the cleaning workflow, merged the to processes together} 
			\end{figure}
\subparagraph\noindent
With this solution design, the high quality data frame, what can be used later is possible and with this idea synthesis, the problems with the previous ones are worked around.
%----------------------------------------------------------------------------
	\subsubsection{Exploring}
	The data exploring and data examination, is a required step for correctly interpret the data and confirm the facts coming from expert knowledge. Without this "null hypothesis check" step, lot of human labor and energy can be thrown out.
		\paragraph{Exploratory data analysis}
			With this steps (data summaries and visualization) one can make himself sure, the data set has some connection with the real world, the values are close to reality and further step are worth a shot.
			\subparagraph{Data summaries}
			There are a lot of data summary methods (mean, median, SQRT, min, max and quarters). With these aggregations ran on the data set, it's get a first impression. It's advisable to use these resource sparing methods.
			\subparagraph{Visualization}
			The human mind can easy interpret large amount of data with correct visual representation and to go further checked the connections and facts derived from expert knowledge. \footnote{Not to mention, amend that knowledge.} 
		\paragraph{Unsupervised learning}\cite{UnsuplearnBook}
		The "unsupervised learning" is a machine learning method especially to unlabeled data, it's useful to find connections and correlation between the values in a higher perspective and confirm or question expert knowledge.

		\subparagraph{k-means clustering}
		\cite{k-means}\cite{kmeans2} The k-means clustering help to estimate groups between the observation, with norming all the values to one and calculate the multi-dimensional distance between them.
		\subparagraph{hierarchical clustering}
		\cite{h-clust}\cite{h-clust2}\cite{Clustering} Is a k-means with a twist, because it clusters clusters hierarchically, step by step with various distance calculation method and top-down or bottom-up approach.
		\subparagraph{Principal component analysis}
		\cite{PCA}
		This algorithm make an orthogonal, multi dimensional space from the values and place there the observations. This algorithm can tell the most significant values from the data set.
%----------------------------------------------------------------------------
	\subsubsection{Specifying Attributes}
	From expert knowledge \footnote{and exploratory data analysis} the main components and indicators of the tire abrasion can be specified.

	These attributes are:
	\begin{itemize}
		\item{elapsed time} 
		\item{traveled distance} 
		\item{count in various speed and torque state}
		\item{changing x direction}
		\item{changing y direction}
		\item{is there weight counter (max 3600 in a hour)}
		\item{steering wheel degree change derived by time aggregated by average}
	\end{itemize}
	All of them will be aggregated hour by hour\footnote{when the tire measurement interval goes down the aggregation window could and should too}, from tire change or measurement.
%----------------------------------------------------------------------------
	\subsubsection{Calculating RUL (Remaining useful life)}
		\paragraph\noindent
	After the specified attributes are calculated and aggregated and enough \footnote{more than one} tire measurement is available with accurate time stamp, the RUL can be calculated in the following way:

	\begin{enumerate}
		\item{join the tire measurement and the hourly aggregated attributes by timestamp}
		\item{interpolate from the tire measurements to the hourly aggregations}
		\item{compute the tire diameter change on all the given measurement points}
		\item{calculate the average diameter change by hourly aggregations} with that the tire diameter change by hourly attribute change is given
		\item{produce the attributes hourly for a new measurement}
		\item{calculate the tire diameter change for all attributes based on proportionality}
		\item{compute the mean tire diameter change by hour}
		\item{summarize the tire diameter change from last know diameter}
		\item{if the summary is greater than a given constant, the tire should be changed}
		\item{compute reaming useful life on the last hours abrasion rate}
	\end{enumerate}
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=150mm, keepaspectratio]{figures/CAN_fingerprint_RUL_calculation.png}
		\caption{Calculating RUL from cleaned data} 
	\end{figure}
		\paragraph\noindent
	Other not implemented solutions:
	\begin{itemize}
		\item{calculate RUL with machine learning} There was not enough data for this solution, the model will be over-fitted based on DataCamp case studies \cite{DataCampCaseStudies} 
		\item{calculate RUL with deep learning} Similar then above.	
	\end{itemize}
	
%----------------------------------------------------------------------------
%----------------------------------------------------------------------------
\section{Electrical fail prediction data set processing method}
%----------------------------------------------------------------------------
	\subsection{Requirement specification}
%----------------------------------------------------------------------------
\paragraph\noindent
From given CAN bus warning collection form the STILL forklifts electrical parts and SAP technical database, which contains the company's technical workers comments on all repair sessions, and occasionally exchanged parts ID and name, and later described warning sequence from a random machine, the model have to estimate the remaining useful life to a next repair session.
\paragraph\noindent
In the scope of this project iteration, the exchange part ID-s is the superior priority, the technical workers comments should be categorized with some natural language processing method. \cite{nlp}
%----------------------------------------------------------------------------
	\subsection{Data processing workflow design}
%----------------------------------------------------------------------------
\paragraph\noindent
As previously has been mentioned the BAL Network science book \cite{BALNWSCBOOK} is an effective indicator of the later specified solution. But a lot of optimizing idea comes from the Computer Science Distilled book\cite{CSDISTILLED}, the usage of the Python language and the NetworkX package\cite{NetworkX} is the influence of the DataCamp e-learning platform \cite{DATACAMP}, and the thinking in flows, process and value streams is the cause of the Lean Thinking book\cite{LeanThinking} and the Phoenix Project book \cite{PhoenixProject}


%----------------------------------------------------------------------------
		\subsubsection{Raw data's condition}
		The data set given for this subproject is consisting three files:
		\begin{itemize}
			\item{xtra\_All\_errors\_with\_metainfo.txt} size of 12.9 GB, contains the warning log of 789 forklifts, with timestamp, machine ID, the ID of the warning, some extra information which is out of the scope of this project\footnote{but in a further iteration could be necessary}, and a lot of NA columns.
			\item{ServiceReportSAPExtrakt\_FLMxFahrzeuge.txt} size of 224.7 MB, contains an export from an SAP repair database, with columns like "when was the machine repair session stared","when was the machine repair session ended", the machine's ID, the exchanged part ID number, some additional information \footnote{not all rows contain exchanged part ID, in these case the technical worker comment can be useful, but the natural language processing is not the scope of this thesis.} and lot of NA columns.
			\item{Alle-SBs-ProActive\_droprows.xlsx} size of 4.5 MB, nearly exactly the same columns like one above, but form different forklifts and a bit different column names.
		\end{itemize}
		\subsubsection{Cleaning in two iteration}
		\begin{itemize}
			\item{Getting rid of NA-s columns, and duplications} This step is required to reduce the further resource cost of the operations, and to have a filebase for further method development.
			\item{Drop not useful columns for the first iteration graph building} For the first iteration graph building, the timestamp, machine ID and the ID of the warning is required from the firstly mentioned file. From the second and the third the repair session starting and ending time, the machine ID and the exchange part ID is required. When there is no exchange part ID just technical worker comment, for the fast method development, it could be handy to give a temporary distinct ID, from the other real exchange part IDs, to these rows exchange part ID column. \footnote{this can be a starting point of the subcategorizing, with the help of the NLP} 
		\end{itemize}
		\subsubsection{Creating nodes list and edges list}
		\paragraph\noindent
		From the previously generated files the nodes and edges, the two components of the graph can be generated. The nodes list should contains all the distinct warning and exchange part ID-s, the edges list should contain the timestamp, the from and to node's ID , and the vehicle ID.
		\paragraph{Making the nodes list}
		The previously generated files contain a lot of duplicated warning and exchange part ID along with a bunch of not needed columns.
		Is this case the planned steps should be:
		\begin{enumerate}
			\item{read in the warning ID and exchange part ID columns from the previously mentioned files} 
			\item{concatenate the two SAP originated lists}
			\item{drop the duplications}
			\item{concatenate the lists}
			\item{save it}
		 \end{enumerate}
		\paragraph{Making the edges list, machine by machine}
		For easier handling and performance and disk space reasons, would be wise to generate the path of all machines.
		The steps should be:
		\begin{enumerate}
			\item{read in the largest file chunk by chunk} 
			\item{group by the information by machine ID}
			\item{save it to a corresponding machine's file}
			\item{do the previous 2 steps to the smaller files after reading it in as a whole}
			\item{read in the distinct machine paths one by one}
			\item{order them by time}
			\item{save them in separated files}
		 \end{enumerate}
		With this step we get the distinct nodes and a lot of edges is different paths. 
		\begin{figure}[!ht]
		\centering
		\includegraphics[width=150mm, keepaspectratio]{figures/Electrical_fail_prediction_preprocessing_workflow.png}
		\caption{The cleaning and ordering step by step} 
		\end{figure}
%----------------------------------------------------------------------------
		\subsubsection{Exploring}
			\paragraph\noindent
			Unlike the previous data set, the cleaning step is not as significant as it could be, because the data had given in a more convenient way. After the later described NA and duplicated column dropping, the data exploring can begin. But the end of the data exploring is after the making of the nodes and edges list, because it's easy to check visually a correct graph representation \footnote{for example: the path is continuous or dashed}.
			\subparagraph{Data summaries}
			There are a lot of data summary methods (mean, median, SQRT, min, max and quarters). With these aggregations ran on the data set, it's get a first impression. 
			\subparagraph{Visualization}
			The human mind can easy interpret large amount of data with correct visual representation and to go further checked the connections and facts derived from expert knowledge. \footnote{Not to mention, amend that knowledge.}
%----------------------------------------------------------------------------
		\subsubsection{Calculating RUL}
			\paragraph{Building the graph}
			When the previous ordering and exploring and fact checking is complete, the graph building can begin, from the generated files, the base is the nodes list all nodes should be represented in the graph, when it comes to edges the situation is different, for computational resource reasons, not all of the path should be included for sure. But the more the path the better be the prediction.
			\paragraph{computing}
			If the graph has been build, the remaining useful life calculation steps should be the following:
			\begin{enumerate}
				\item{get a warning sequence, what is not represented in the graph, from the same machine, in a chronological order}
				\item{between the oldest and the second oldest nodes given in the sequence and the graph \footnote{all the nodes in the new sequence should be int th graph too} list the machine ID-s on that path}
				\item{iterate through the hole sequence and eliminate the machines what are absent in the current edges}
				\item{when there is no machine shorted the sequence with eliminate the oldest node in the sequence and redo the previous search}
				\item{the list of the machined with similar path are given, from the last and youngest node in the sequence the shortest similar machine path should be listed to the exchange part ID nodes}
				\item{from the last node the possibility could be calculated to all exchange part ID nodes, based on the similar shortest machine path count to the specific exchange part ID nodes divided by sum of to all the exchange part ID nodes}
				\item{besides of possibility the time to the specific exchange part ID node could be computed to give more in sight about the remaining useful life, based on the similar shortest machine's distance in time}
				\item{the result should look like a with columns like: exchange part ID node, min to that node, mean, median and max, and the computed possibility arranged by possibility and time} 
		 		\item{the RUL is the highest possibility with the shortest remaining time}
		 	\end{enumerate}
			
		The calculation of the whole table is profitable, because the technical workers can solve more problem at one repair sessions, this could cause longer timespan on duty.

		TODO explain red flags in short
		\begin{figure}[!ht]
		\centering
		\includegraphics[width=150mm, keepaspectratio]{figures/Calc_Fault_pred_RUL.png}
		\caption{To optimize the cleaning workflow, merged the to processes together} 
		\end{figure}