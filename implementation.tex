%----------------------------------------------------------------------------
\chapter{Implementation}
%----------------------------------------------------------------------------
\section{CAN fingerprint data set processing method}
\paragraph\noindent
This sections would describe the algoritmization and realization of the previously described process in the design chapter in R language.The reasons are simple, the R is one of the most common and convenient tools to process data sets and interpret them, with a fitted statistical or machine learning model.
%----------------------------------------------------------------------------
\subsection{Cleaning}
%----------------------------------------------------------------------------
		\subsubsection{Idea One}
		\paragraph\noindent
		In this first iteration, the Idea One, described under the similar section in the design chapter, has been algorithmically implemented in R language.

		The following R packages were used:
		\begin{itemize}
			\item{R.matlab:} This package was used to handle the *.mat files, and convert column to a usable format.
			\item{dplyr,purrr,plyr:} These three package are part of the tidyverse \cite{Tidyverse}, these are excellent for handling as especially cleaning data frames. This  advantage was used for multiple times in the implementation process. The tidyverse R package has one more benefit: The contained packages are written in C++ language, what gives a speed edge against the regular packages.
			\item{lubridate:} Also from the the tidyverse meta package, although with a distinct purpose. This package is beneficial to handle time series and time related operation. With the MATLAB export files and timestamp key this is a needed unsavory.
			\item{stringr:} This package is from the tidyverse as well, with the purpose of handling strings. Some of the function needed in the cleaning process automatization.
		\end{itemize}

		\paragraph\noindent
		The cleaning algorithm is capable:
		\begin{itemize}
			\item{To handle multiple MATLAB export files from a single folder}
			\item{To change the German column names to English} 
			\item{To boxshort all the columns in one file based on the key-value pairs in a column} 
			\item{To diminish not useful columns for future operations} 
			\item{To interpolate between measurement point to make a dataframe without NA-s}
			\item{To concatenate the time related columns into one} 
			\item{To save the cleaned files in RDS\footnote{R Data Structure} format} 
		\end{itemize} 

		\paragraph\noindent
		The actual algorithms' implementation with these packages were slightly, this algorithm described in the already mentioned section were successfully implemented. However the performance were humble with the small files already. \footnote{See in the verification chapter in more detail.} For the big MATLAB exports, the performance were close to zero, because the cleaning process were not completed in half a day.
		
		These result caused the need for the second iteration on the cleaning process and the necessity for new ideas.
%----------------------------------------------------------------------------
		\subsubsection{Idea Two}
		The second idea was to take advantage of constant sensor frequency. This phenomenon is causing constant gaps between the columns key value pairs. The constant gap cause constant insertion volume for a column and faster interpolation.

		This idea were also implemented and the cleaning process' performance were remarkable compared to the previous one. However, as the second cleaning algorithm were tested, the data could contain whole spanning about minutes wide. This event completely ruins the process. The next step were clear, there is a need for and another cleaning solution with the avail of the two current ones.
%----------------------------------------------------------------------------
		\subsubsection{Idea Synthesis}
		After redefining the corner principles of the cleaned data set, the synthesis of the two previous ideas were possible.
		There is no need for observation in every hundredth of a second, just in second in the first place. This approach allows fast aggregation and eliminate the need for slow interpolation in this level.

		After the aggregation of the values is done by second to second and the data is arranged in the by the key, it's easy to join them to a previously build data frame. After that step can come the interpolation to close the minute span gaps.

		That is what were implemented as the final cleaning process. \footnote{In more detail, see the verification chapter.}
%----------------------------------------------------------------------------		
\subsection{Data processing and understanding}
		The data processing and understanding in depth is an essential step for developing a proper data handling, and predicting model.
		The reason is seems trivial, however, it's easy to overlook.When the data scientist don't understand the data it could cause wrong models. This induces the need for exploratory data analysis.
%----------------------------------------------------------------------------
	\subsubsection{Exploratory data analysis}
		Exploratory data analysis is the tool set, what help to catch the underlying principles and connection hidden between the values. It's advantageous to involve this step in the predicting model developing process. In this case new principles and connections can be found, and the old ones derived from the expert knowledge can be verified.
%----------------------------------------------------------------------------
		\paragraph{Data summaries}
		When the cleaning is ready, the easiest, and fastest way to get the first impression about the given data set is the build in data summaries and some from the tidyverse package.

		The most frequently used data summary functions in this thesis:
		\begin{itemize}
			\item{head:} To look at the first few rows of the data table or list.
			\item{glimpse:} To look at the first few rows of the data table or list. However in a more eye friendly and informative way.\footnote{From the tidyverse.}
			\item{summary:} To receive summary statistic from each row.
			\item{describe:} To receive fast summary statistic from each row. However in also a more eye friendly and informative way.\footnote{Also from the tidyverse.}
		\end{itemize} 
%----------------------------------------------------------------------------		
		\paragraph{Visualization}
		The human mind could process the information visually in a fastest way, make assumptions easily, and find additional connections. 

		To fulfill this purpose two package had used from the R language tool set:
		\begin{itemize}
			\item{base:} This package is an basic tool for plotting, and there is no need for additional package for most visualization task. However this not a beneficial tool, because the visualization code isn't as reusable and convenient as it should be for this scale of repetitions. The reasons is simple code design principles were not on a high level when this package had written.
			\item{ggplot2:} In contrast, ggplot2 is also from the tidyverse and because of this reason also written in C++, which guarantee the necessary speed for when it's high demand. Moreover, this package was designed with advanced principles and the code is mostly reusable, and handles data frames efficiently. 
		\end{itemize} 

		\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/clusteringf.png}
			\caption{ToDo} 
			\end{figure}
		\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/CrashYvsSteeringangle.png}
			\caption{ToDo} 
			\end{figure}
		\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/Rplot.png}
			\caption{ToDo} 
			\end{figure}		
		\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/speedvstorquefacet.png}
			\caption{ToDo} 
			\end{figure}	
		\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/timevssteeringangle.png}
			\caption{ToDo} 
			\end{figure}	
		\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/torquespeedcat.png}
			\caption{ToDo} 
			\end{figure}		
		confirm expert knowledge
%----------------------------------------------------------------------------		
	\subsubsection{Unsupervised learning}
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/clustering.png}
			\caption{ToDo} 
			\end{figure}
	description and why some of that is used.
	confirm expert knowledge
	\#WHY
%----------------------------------------------------------------------------	
\subsection{Calculating Attributes}
	from expert knowledge, why does the solution have worked out this way
	\#WHY
%----------------------------------------------------------------------------	
\subsection{Calculating RUL (Remaining useful life)}
	model description in detail.
	\#WHY
%----------------------------------------------------------------------------	
\subsection{Optimization}
Possible optimizations enumerate.
\#WHY
\subsection{Results}
Estimating result, and what causing aleatory mistakes in the model. 
\#WHY
\cite{GitHub_CAN_RUL}
%----------------------------------------------------------------------------
%----------------------------------------------------------------------------
\section{Electrical fail prediction data set processing method}
\paragraph\noindent
This sections would describe the algorithmization and realization of the previously described process in the design chapter in Python language.
\#WHY
%----------------------------------------------------------------------------
	\subsection{Cleaning}
Cleaning the data set in detail, why I made these choices with that In the graph target in mind
\#WHY
		\#TODO use github and describe the algorithm
%----------------------------------------------------------------------------
	\subsection{Exploring}
		\paragraph{Data summaries}
		Common Types and applied types
		confirm expert knowledge
		\#WHY
%----------------------------------------------------------------------------		
		\paragraph{Visualization}
		Common Tool and applied tool
			\begin{figure}[!ht]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/fault_pred_files/path.png}
			\caption{ToDo} 
			\end{figure}
		confirm expert knowledge
\#WHY
%----------------------------------------------------------------------------
	\subsection{Calculating RUL}
model description in detail.
\#WHY
%----------------------------------------------------------------------------
	\subsection{Optimization}
Possible optimizations enumerate.
\#WHY
%----------------------------------------------------------------------------
	\subsection{Result}
Estimating result, and what causing aleatory mistakes in the model. 
\#WHY
\cite{GitHub_FP_RUL}