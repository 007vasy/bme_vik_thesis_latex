%----------------------------------------------------------------------------
\chapter{Implementation}
%----------------------------------------------------------------------------
\section{CAN fingerprint data set processing method}
\paragraph\noindent
This sections would describe the algoritmization and realization of the previously described process in the design chapter in R language.The reasons are simple, the R is one of the most common and convenient tools to process data sets and interpret them, with a fitted statistical or machine learning model.
%----------------------------------------------------------------------------
\subsection{Cleaning}
%----------------------------------------------------------------------------
		\subsubsection{Idea One}
		\paragraph\noindent
		In this first iteration, the Idea One, described under the similar section in the design chapter, has been algorithmically implemented in R language.

		The following R packages were used:
		\begin{itemize}
			\item{R.matlab:} This package was used to handle the *.mat files, and convert column to a usable format.
			\item{dplyr,purrr,plyr:} These three package are part of the tidyverse \cite{Tidyverse}, these are excellent for handling as especially cleaning data frames. This  advantage was used for multiple times in the implementation process. The tidyverse R package has one more benefit: The contained packages are written in C++ language, what gives a speed edge against the regular packages.
			\item{lubridate:} Also from the the tidyverse meta package, although with a distinct purpose. This package is beneficial to handle time series and time related operation. With the MATLAB export files and timestamp key this is a needed unsavory.
			\item{stringr:} This package is from the tidyverse as well, with the purpose of handling strings. Some of the function needed in the cleaning process automatization.
		\end{itemize}

		\paragraph\noindent
		The cleaning algorithm is capable:
		\begin{itemize}
			\item{To handle multiple MATLAB export files from a single folder}
			\item{To change the German column names to English} 
			\item{To boxshort all the columns in one file based on the key-value pairs in a column} 
			\item{To diminish not useful columns for future operations} 
			\item{To interpolate between measurement point to make a dataframe without NA-s}
			\item{To concatenate the time related columns into one} 
			\item{To save the cleaned files in RDS\footnote{R Data Structure} format} 
		\end{itemize} 

		\paragraph\noindent
		The actual algorithms' implementation with these packages were slightly, this algorithm described in the already mentioned section were successfully implemented. However the performance were humble with the small files already. \footnote{See in the verification chapter in more detail.} For the big MATLAB exports, the performance were close to zero, because the cleaning process were not completed in half a day.
		
		These result caused the need for the second iteration on the cleaning process and the necessity for new ideas.
%----------------------------------------------------------------------------
		\subsubsection{Idea Two}
		The second idea was to take advantage of constant sensor frequency. This phenomenon is causing constant gaps between the columns key value pairs. The constant gap cause constant insertion volume for a column and faster interpolation.

		This idea were also implemented and the cleaning process' performance were remarkable compared to the previous one. However, as the second cleaning algorithm were tested, the data could contain whole spanning about minutes wide. This event completely ruins the process. The next step were clear, there is a need for and another cleaning solution with the avail of the two current ones.
%----------------------------------------------------------------------------
		\subsubsection{Idea Synthesis}
		After redefining the corner principles of the cleaned data set, the synthesis of the two previous ideas were possible.
		There is no need for observation in every hundredth of a second, just in second in the first place. This approach allows fast aggregation and eliminate the need for slow interpolation in this level.

		After the aggregation of the values is done by second to second and the data is arranged in the by the key, it's easy to join them to a previously build data frame. After that step can come the interpolation to close the minute span gaps.

		That is what were implemented as the final cleaning process. \footnote{In more detail, see the verification chapter.}
%----------------------------------------------------------------------------		
\subsection{Data processing and understanding}
		The data processing and understanding in depth is an essential step for developing a proper data handling, and predicting model.
		The reason is seems trivial, however, it's easy to overlook.When the data scientist don't understand the data it could cause wrong models. This induces the need for exploratory data analysis.
%----------------------------------------------------------------------------
	\subsubsection{Exploratory data analysis}
		Exploratory data analysis is the tool set, what help to catch the underlying principles and connection hidden between the values. It's advantageous to involve this step in the predicting model developing process. In this case new principles and connections can be found, and the old ones derived from the expert knowledge can be verified.
%----------------------------------------------------------------------------
		\paragraph{Data summaries}
		When the cleaning is ready, the easiest, and fastest way to get the first impression about the given data set is the build in data summaries and some from the tidyverse package.

		The most frequently used data summary functions in this thesis:
		\begin{itemize}
			\item{head:} To look at the first few rows of the data table or list.
			\item{glimpse:} To look at the first few rows of the data table or list. However in a more eye friendly and informative way.\footnote{From the tidyverse.}
			\item{summary:} To receive summary statistic from each row.
			\item{describe:} To receive fast summary statistic from each row. However in also a more eye friendly and informative way.\footnote{Also from the tidyverse.}
		\end{itemize}

		\footnote{The data summaries where not included, because the visualizations are more informative for a new eye.}
%----------------------------------------------------------------------------		
		\paragraph{Visualization}
		The human mind could process the information visually in a fastest way, make assumptions easily, and find additional connections. 

		To fulfill this purpose two package had used from the R language tool set:
		\begin{itemize}
			\item{base:} This package is an basic tool for plotting, and there is no need for additional package for most visualization task. However this not a beneficial tool, because the visualization code isn't as reusable and convenient as it should be for this scale of repetitions. The reasons is simple code design principles were not on a high level when this package had written.
			\item{ggplot2:} In contrast, ggplot2 is also from the tidyverse and because of this reason also written in C++, which guarantee the necessary speed for when it's high demand. Moreover, this package was designed with advanced principles and the code is mostly reusable, and handles data frames efficiently. 
		\end{itemize} 


		\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/clusteringf.png}
			\caption{As it seems, the density function of the hydraulic pressure are different in the different types of tracks. It's implicates different paths cause different encumbrance, ergo different tire abrasion.} 
			\end{figure}
		\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/CrashYvsSteeringangle.png}
			\caption{On this figure the Y-way accelerometer on the X-axis and the Steering wheels' angle on the Y-axis. From this appear the linear connection between the two value, and it's useful for crosschecking.} 
			\end{figure}
		\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/Rplot.png}
			\caption{This visualizations tells the mean hydraulic pressure when there is load on the machine. The blurry oscillation is caused by rough terrain and obstacles on the road.} 
			\end{figure}		
		\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/speedvstorquefacet.png}
			\caption{The different driving profiles principle can be verified with this figure. The distinct speed-torque pairs cause different tire abrasion.} 
			\end{figure}	
		\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/timevssteeringangle.png}
			\caption{The different driving profiles principle can be verified with this figure too. The distinct turn types cause different tire abrasion.} 
			\end{figure}	
		\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/torquespeedcat.png}
			\caption{The connection between the two drivemotors speed and torque across the fingerprint files.} 
			\end{figure}		
%----------------------------------------------------------------------------		
	\subsubsection{Unsupervised learning}
	The unsupervised learning is efficient to handling and examining unlabeled data in the first place.
	
	In this particular case it used to check the expert knowledge what says there is three driving profiles.
	\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/clustering.png}
			\caption{Clustering the speed and torque pairs with a hierarchical bottom-up clustering.} 
	\end{figure}
	As it seems from this figure, the expert knowledge was partly wrong, because they forgot to mention the fourth state: the standin still state.

	This can be argued further, because if the machine standing still with a load it damages the tire more than moving slowly with low torque.
%----------------------------------------------------------------------------	
\subsection{Calculating Attributes}
As already have been mentioned in the design section the attributes are coming from the experts' knowledge and exploratory data analysis.

The attributes are allocated hourly, because the tire abrasion is a relative slow process and it would be disadvantageous to go into higher time resolution.

These attributes have been calculated from the cleaned data and aggregated hourly:
	\begin{itemize}
		\item{elapsed time:} From the starting point where is the last tire measurement was in time.
		\item{traveled distance:} Calculated second by second then aggregated hourly using the derivative distance formula: $s = dv*dt + 1/2*da*dt^2$.
		\item{count in various speed and torque state:} By reason of the different driving profiles, various speed and torque pairs must be categorized second by second and summed hourly. The categorization is done by normalization and 2D binning, with the help of the dplyr package.
		\item{changing x direction:} Calculated second by second, if the previous value of the speed was different than the current changing in x direction happened. This also summed hourly.
		\item{changing y direction:} Same as above. However with a twist, for this one secondary variable have been calculated if the machine goes in a straight line this secondary value is TRUE.
		\item{is there weight counter (max 3600 in a hour):} If the minutely mean hydraulic pressure is over the constants from the visualization there is load on the truck. This us also collected hourly.
		\item{steering wheel degree change derived by time aggregated by average:}The derivation is made second by second, the summary is calculated hourly.
		\footnote{the torque and speed derived by time could be useful too}
	\end{itemize}
%----------------------------------------------------------------------------	
\subsection{Calculating RUL (Remaining useful life)}
	After the previous step had been done, and the attributes are calculated, the remaining useful life calculation can begin.
	The hourly aggregated attributes are in a dataframe without the sensory values. A new columns have generated with NA-s and the tire measurements are imported with multiple measurement points. Moreover, inserted to the new columns next to the appropriate hourly timestamp key. When it's done the interpolation should begin to all aggregations points. \footnote{The interpolation was linear. Thus it could be more precise when the interpolation is based on some metascore the elapsed time and travelled distance involved, or with some machine learning algorithm.}

	Whit this step and more precise tire measurements on multiple occasions, the aggregated values can indicate a mean tire abrasion change. If that change is more than specified constant the tire should be changed. From the last hour a tire abrasion rate was calculated \footnote{Based on elapsed time} and with that the remaining useful life had calculated.
	\#Enumerate the steps?
%----------------------------------------------------------------------------	
\subsection{Results}
The results were statistically promising. However, for more precise results, there is a claim for more tire measurement.

The code can be viewed on the following GitHub repository:\cite{GitHub_CAN_RUL}
%---------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------
\section{Electrical fail prediction data set processing method}
\paragraph\noindent
As it already has been mentioned for handling this dataset, the Python language was used. Especially, for the easy data representation and handling with the NetworkX package. 
%----------------------------------------------------------------------------
	\subsection{Cleaning}
For this data set the cleaning was far more lighter than the previous one. The data is represented in tabular separated txt and in xlsx file formats.
To progress further two steps were made:
\begin{itemize}
			\item{dropping the NA and duplicated columns}
			\item{dropping the meaningless columns for the first iteration graph building}
\end{itemize}

At the end of this process, only the following columns were kept from the files:
\begin{itemize}
			\item{file containing the electric errors}
			\begin{itemize}
				\item{s\_errorcode}
				\item{vehicle\_serialnumber}
				\item{error\_occurred\_timestamp}
			\end{itemize}
			\item{file containing the SAP fails}
			\begin{itemize}
				\item{exchange part number}
				\item{vehicle\_serialnumber}
				\item{service task(s) begin}
				\item{service task(s) end}
			\end{itemize}
\end{itemize}
These views were saved in a distinct files for performance and milestone reasons.
%----------------------------------------------------------------------------
	\subsection{Creating nodes list and edges list}
From the previously generated files the nodes and edges list were generated in a following way.
\paragraph{Nodes list generation}
The nodes list contains all the distinct s-errorcodes from the electric error files once, and the distinct exchange par numbers once from the SAP fail file. The condition space could be built.
\paragraph{Edges list generation}
The edges list was generated from three sources: 
\begin{itemize}
	\item{the electric errors} with all three columns
	\item{the SAP fails once} with all columns except the service task(s) end timestamp
	\item{the SAP fails once more with a twist} with all columns except the service task(s) begin timestamp
\end{itemize}

This dataframe \footnote{Containing three columns} was processed further. The dataframe's rows were grouped by the vehicle ID, were arranged by the timestamps.
\footnote{The similar rows with close timestamp weren't been eliminated.}
The directed edge needs a starting and ending point and to achieve that purpose the starting states were derived from the previous row ending state by vehicle ID, and after that the data was saved in distinct files by vehicle ID.
%----------------------------------------------------------------------------
	\subsection{Exploring}
		\paragraph{Data summaries}
		With this special dataset, the common data summary functions are nearly useless, because the lack of quantitative variables, and the condition state representation.
		The useful data summaries are length of the dissimilar edges' and nodes' lists, because it helped to confirm the quality of the data for the graph representation and RUL calculation. 

%----------------------------------------------------------------------------		
		\paragraph{Visualization}
		The visualizations are beneficial when it comes to a graph represented data. In this project i had used a the NetworkX build in graph building and visualization, and the matplotlib package plotting functions.

		As is seems it's difficult to make a guess but it can be seen the opportunity to statistically used the states and the edges' weight to make a prediction.
			\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/fault_pred_files/path.png}
			\caption{ToDo} 
			\end{figure}
		\#TODO directed graph
\#WHY

%----------------------------------------------------------------------------
	\subsection{Calculating RUL}
		\subsubsection{Calculating RUL}
			\paragraph{Building the graph}
			When the previous ordering and exploring and fact checking is completed, the graph building can begin from generated files. Thus, the base is the nodes list, moreover all nodes should be represented in the graph when it comes to edges the situation is different. For computational resource reasons, not all of the paths should be included for sure. But the more the path the better be the prediction.
			\paragraph{Computing steps:}
			If the graph has been build, the remaining useful life calculation steps should be the following:
			\begin{enumerate}
				\item{getting a warning sequence, what is not represented in the graph, from the same machine, in a chronological order}
				\item{the machine' IDs should be listed between the oldest and the second oldest nodes given in the sequence} \footnote{all the nodes in the new sequence should be int th graph too} 
				\item{iterating through the whole sequence and eliminating the machines which are absent in the current edges}
				\item{when there is no machine shorted the sequence with, eliminate the oldest node in the sequence and redo the previous search}
				\item{the list of machines with similar path are given, from the last and youngest node in the sequence. The shortest similar machine path should be listed to the exchange part ID nodes}
				\item{from the last node the possibility could be calculated to all exchange part ID nodes, based on the similar shortest machine path count to the specific exchange part ID nodes divided by sum of to all the exchange part ID nodes}
				\item{besides of possibility the time to the specific exchange part ID node could be computed to give more insights about the remaining useful life, based on the similar shortest machine's distance in time}
				\item{the result should look with columns like: exchange part ID node, minimal, mean, median and maximal time to that node, and the computed possibility arranged by possibility and time} 
		 		\item{the RUL is the highest possibility with the shortest remaining time}
		 	\end{enumerate}
%----------------------------------------------------------------------------
	\subsection{Result}
The calculation of the whole table is profitable, because the technical workers can solve more problem at one repair sessions, this could cause longer timespan on duty.

Estimating result, and what causing aleatory mistakes in the model. 
\#WHY

The code can be viewed on the following GitHub repository:\cite{GitHub_FP_RUL}