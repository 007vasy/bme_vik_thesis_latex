%----------------------------------------------------------------------------
\chapter{Implementation}
%----------------------------------------------------------------------------
\section{CAN fingerprint dataset processing method}
\paragraph\noindent
This chapter will describe the algorithmization and realization of the process presented previously in the Design chapter, in R language. The reasons are simple, the R is one of the most common and convenient tool to process datasets and interpret them with a fitted statistical or machine learning model.
%----------------------------------------------------------------------------
\subsection{Cleaning}
%----------------------------------------------------------------------------
		\subsubsection{Idea One}
		\paragraph\noindent
		In this first iteration, the Idea One, described under the similar section of the Design chapter, had been algorithmically implemented in R language.

		The following R packages were used:
		\begin{itemize}
			\item{R.matlab:} This package was used to handle the *.mat files, and convert columns to a usable format.
			\item{dplyr, purrr, plyr:} These three packages are part of the tidyverse \cite{Tidyverse}, and excellent for handling as especially cleaning data frames. This  advantage was used for multiple times in the implementation process. The tidyverse R package has one additional benefit: the contained packages are written in C++ language, which gives a speed edge against the regular packages.
			\item{lubridate:} It is also from the tidyverse meta package, although with a distinct purpose. This package is beneficial to handle time series and time related operation. With the MATLAB export files and timestamp key this is a needed unsavoury.
			\item{stringr:} This package is from the tidyverse as well, with the purpose of handling strings. Some of the functions are needed in cleaning process automatization.
		\end{itemize}

		\paragraph\noindent
		The cleaning algorithm is capable:
		\begin{itemize}
			\item{to handle multiple MATLAB export files from a single folder,}
			\item{to change German column names to English,} 
			\item{to boxshort all columns in one file based on key-value pairs in a column,} 
			\item{to diminish not useful columns for future operations,} 
			\item{to interpolate between measurement points to make a dataframe without NA-s,}
			\item{to concatenate time related columns into one,} 
			\item{to save cleaned files in RDS\footnote{R Data Structure} format.} 
		\end{itemize} 

		\paragraph\noindent
		The actual algorithms' implementation with these packages were not feasible. This algorithm described in the already mentioned section were successfully implemented. However, the performance were humble with the small files already. \footnote{See in the verification chapter more in detail.} For the significant MATLAB exports, the performance was close to zero, because the cleaning process was not completed in half a day.
		
		These results caused a need for the second iteration in the cleaning process and necessity for new ideas.
%----------------------------------------------------------------------------
		\subsubsection{Idea Two}
		The second idea was to take advantage of constant sensor frequency. This phenomenon was causing constant gaps between the columns' key-value pairs. The constant gaps resulted constant insertion volume for a column and faster interpolation.

		This idea was also implemented and the cleaning process' performance was remarkable compared to the previous one. However, as the second cleaning algorithm was tested, the data contained holes, spanned about minutes wide. This fact completely ruined the process. The next steps were clear, there was a need for another cleaning solution with the avail of the two current ones.
%----------------------------------------------------------------------------
\clearpage\subsubsection{Idea Synthesis}
		After redefining the main principles of the cleaned dataset, the synthesis of the two previous ideas were possible.
		There is no need for observation in every 10 milliseconds, just in a second in the first place. This approach allows significantly faster aggregation and eliminates the need for slow interpolation in this level.

		The aggregation of values are done second by second and the data was arranged by the key (timestamp). It is easy to join them to a previously built data frame\footnote{The previously built data frame represents the boxes for the box-short algorithm, and the sorting was based on the keys.}. After that step, the interpolation came, to close the minute-wide gaps.

		This was implemented as the final cleaning process\footnote{See the results in the verification chapter more in detail.}.
%----------------------------------------------------------------------------		
\subsection{Data processing and understanding}
		The data processing and understanding in depth is an essential step for developing a proper data handling and predicting model.
		The reasons are trivial, however, it is easy to pass by. When a data scientist do not understand the data, it could cause wrong models. This induces the need for exploratory data analysis.
%----------------------------------------------------------------------------
	\subsubsection{Exploratory data analysis}
		Exploratory data analysis is the tool set that help to catch the underlying principles and connections hidden between the values. It is advantageous to involve this step in the predicting model's developing process. In this case new principles and connections can be found and old ones derived from the experts' knowledge can be verified.
%----------------------------------------------------------------------------
		\paragraph{Data summaries:}
		When the cleaning is ready, the easiest and fastest way to get the first impression about the given dataset is the build in data summaries and some from the tidyverse package.

		The most frequently used data summary functions in this thesis\footnote{The data summaries where not included, because the visualizations are more informative for a new eye.} are:
		\begin{itemize}
			\item{head:} to look at the first few rows of a data table or list.
			\item{glimpse:} to look at the first few rows of a data table or list. However, in a more eye friendly and informative way\footnote{From the tidyverse.}.
			\item{summary:} to receive summary statistics from each row.
			\item{describe:} to receive fast summary statistic from each row. ,in  a more eye friendly and informative way\footnote{Also from the tidyverse.}.
		\end{itemize}
%----------------------------------------------------------------------------		
		\paragraph{Visualizations:}
		A human mind could process information visually in the fastest way, make assumptions easily, and find additional connections. 

		To fulfil this purpose two package had been used from the R language tool set:
		\begin{itemize}
			\item{base:} This package is a basic tool for plotting, and there is no need for additional package for most visualization tasks. However, this is not a beneficial tool, because the visualization code is not as reusable and convenient as it should be for this scale of repetitions. The reasons are simple, code design principles were not on high level when this package had written.
			\item{ggplot2:} On the contrary, ggplot2 is also from the tidyverse and because of this reason, it is also written in C++, which guarantees the necessary speed for when it is high demand. Moreover, this package was designed with advanced principles, and the code is mostly reusable, and handles data frames efficiently. 
		\end{itemize} 

		\subparagraph{The generated figures for exploratory data analysis are listed below:}
		\begin{itemize}
			\item{Lifting hydraulic pressure density functions, separated by distinct file:} Pressure on the x-axis, density on the y-axis. See Figure \textbf{\ref{implementation:pressure}} .
			\item{Crash Y and steering wheel degree correlation, for cross checking. Figure \textbf{\ref{implementation:Crash}} .}
			\item{Hydraulic pressure levels by file, time on x-axis. Figure \textbf{\ref{implementation:loadon}} .}
			\item{Speed versus torque by file. Different driving profiles. Figure \textbf{\ref{implementation:svst}} .}
			\item{Steering wheel angle versus time by file. Different driving profiles. time on the x-axis. Figure \textbf{\ref{implementation:timevssteeringangle}} .}
			\item{One fingerprint displayed to find correlation and for data validation. Figure \textbf{\ref{implementation:tands}} .}
		\end{itemize}

		\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/clusteringf.png}
			\caption{As it seems, the density functions of hydraulic pressure (x-axis) are different in the different types of tracks. It implicates that different paths are causing distinct encumbrance, ergo different tire abrasion.\label{implementation:pressure}} 
			\end{figure}
		\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/CrashYvsSteeringangle.png}
			\caption{On this figure, the Y-way accelerometer is on the X-axis and the Steering wheels' angle is on the Y-axis. From this, the linear connection between the values are visible, and it is useful for cross-checking.\label{implementation:Crash}} 
			\end{figure}
		\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/Rplot.png}
			\caption{These visualizations tell the mean hydraulic pressure (y axis) when there is load on the machine. The blurry oscillation is caused by rough terrain and obstacles on the road. Time on the x-axis.\label{implementation:loadon}} 
			\end{figure}		
		\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/speedvstorquefacet.png}
			\caption{The different driving profiles principle can be verified with this figure. The distinct speed-torque pairs cause different tire abrasion.\label{implementation:svst}} 
			\end{figure}	
		\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/timevssteeringangle.png}
			\caption{The different driving profiles principle can be verified with this figure as well. The distinct turn types cause different tire abrasion.\label{implementation:timevssteeringangle}} 
			\end{figure}	
		\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/torquespeedcat.png}
			\caption{The connection between the two driver motors speed and torque across the fingerprint files.\label{implementation:tands}} 
			\end{figure}		
%----------------------------------------------------------------------------		
	\subsubsection{Unsupervised learning}
	Unsupervised learning is efficient for handling and examining unlabelled data in the first place.
	
	In this particular case it is used to check the experts' knowledge what says there are three driving profiles.
	\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/can_f/clustering.png}
			\caption{Clustering speed and torque pairs with a hierarchical bottom-up clustering.\label{fig:clustering}} 
	\end{figure}
	As it is visible, from this figure \textbf{\ref{fig:clustering}} , the experts' knowledge was partly wrong, because they forgot to mention the fourth state: the standing still state.

	This can be argued further, because if the machine is standing still with a load, it damages the tire more than moving slowly with low torque.
%----------------------------------------------------------------------------	
\newpage
\subsection{Calculating attributes}
As it has already been mentioned in the design section, the attributes are coming from the experts' knowledge and exploratory data analysis.

The attributes were allocated hourly, because the tire abrasion is a relatively slow process and it would be disadvantageous to go into higher time resolution.

These attributes have been calculated from the cleaned data and aggregated hourly:
	\begin{itemize}
		\item{elapsed time:} From the starting point where the last tire measurement was in time.
		\item{traveled distance:} It is calculated second by second then aggregated hourly using the derivative distance formula: $s = dv*dt + 1/2*da*dt^2$.
		\item{count in various speed and torque state:} By reasons of the different driving profiles, various speed and torque pairs must be categorized second by second and summed hourly. The categorization was done by normalization and 2D binning with the help of the dplyr package.
		\item{changing x direction:} It is calculated second by second. If the previous values of the speed were different from the current, than changing in x direction happened. This was also summed hourly.
		\item{changing y direction:} Same as above. However, with a twist. For this one, a secondary variable have been calculated if the machine goes in a straight line, this secondary value is TRUE.
		\item{"is there weight" counter (max 3600 in an hour):} If the minutely mean hydraulic pressure is over the constants from the visualization, there is load on the truck. This was also collected hourly.
		\item{steering wheel degree change derived by time aggregated by average:} The derivation is made second by second, the summary is calculated hourly\footnote{torque and speed derived by time could be useful, as well}.
	\end{itemize}
%----------------------------------------------------------------------------	
\subsection{Important Note:}
			The STILL company, due to more important projects and tasks, not provided enough CAN-bus data between two tire measurement points (2017.12.08.). Thus, the following lines are just implementation plans, and the functions couldn't be tested for real by the due date of the thesis.

			To solve this situation, I was written a recursive data multiple function , but it was not enough time to test the solution \cite{rekur}.

\subsection{Calculating RUL (Remaining Useful Life)}
	After the previous steps have been done, and the attributes were calculated, the remaining useful life calculation can begin.
	The hourly aggregated attributes were in a dataframe without sensory values. A new columns has generated with NA-s \footnote{To have a place for the interpolation.} and the tire measurements were imported with multiple measurement points. Moreover, there were inserted to the new columns next to the appropriate hourly timestamp key. When it is done, the interpolation should begin to all aggregations points\footnote{The interpolation was linear. Thus, it could be more precise when the interpolation is based on some meta score involving elapsed time and travelled distance, or with some machine learning algorithm.}.

	Whit this step and more precise tire measurements on multiple occasions, the aggregated values can indicate a mean tire abrasion change. If that change was more than a specified constant, the tire should be changed. From the last hour a tire abrasion rate was calculated\footnote{Based on elapsed time} and with that the remaining useful life had calculated.
	
	The RUL calculation process steps are the following:
	\begin{enumerate}
		\item{Initial state:} The hourly aggregated attributes were in a dataframe without sensory values. 
		\item{Tire measurement imported and merged to the dataframe by timestamp key.}
		\item{The tire measurements was interpolated to all observation.}
		\item{New columns with NA-s was generated to the calculated RULs}, based on the count of the tire measurement points and attributes count\footnote{New columns count is equal to the tire measurement points $*$ attributes count.}
		\item{Tire abrasion changing rates are calculated by attributes to the previously generated column}
		\item{Based on the last aggregation window mean abrasion rate, the RUL are calculable.} Just a constant tire abrasion limit is needed\footnote{These results are usable to predict other machines tire abrasion, if the attributes are calculated and aggregated in a similar time window.}.
	\end{enumerate}
		
	The whole CAN fingerprint calculation workflow is in the Appendix \textbf{\ref{appendix:CANFPRULCalc}} .
%----------------------------------------------------------------------------	
\subsection{Results}
The results were statistically a promise. However, for more precise results there is a claim for more tire measurement.

The code can be viewed on my GitHub repository \cite{GitHub_CAN_RUL}.
%---------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------
\paragraph\noindent
\section{Electrical failure prediction dataset processing method}
As it already had been mentioned, for handling this dataset, the Python language was used. Especially, for easy data representation and handling with the NetworkX package. 

The electrical failure prediction preprocessing workflow is in the Appendix \textbf{\ref{appendix:FaultPredPreProc}} .


%----------------------------------------------------------------------------
	\subsection{Cleaning}
For this dataset the cleaning was far easier than the previous one. The data is represented in tabular separated txt and in xlsx file formats.
To progress further, two steps were made:
\begin{itemize}
			\item{dropping the NA and duplicated columns,}
			\item{dropping the meaningless columns for the first iteration graph building.}
\end{itemize}

At the end of this process, only the following columns were kept from the files:
\begin{itemize}
			\item{file containing the electric errors,}
			\begin{itemize}
				\item{s\_errorcode,}
				\item{vehicle\_serialnumber,}
				\item{error\_occurred\_timestamp.}
			\end{itemize}
			\item{file containing the SAP fails.}
			\begin{itemize}
				\item{exchange part number,}
				\item{vehicle\_serialnumber,}
				\item{service task(s) begin,}
				\item{service task(s) end.}
			\end{itemize}
\end{itemize}
These views were saved in a distinct file for performance and milestone reasons.
%----------------------------------------------------------------------------
	\subsection{Creating nodes' list and edges' list}
From files created previously, the nodes' and edges' lists were generated in the following way.
\paragraph{Nodes' list generation:}
The nodes' list contains all the distinct s-errorcodes from the electric error files once, and the distinct exchange par numbers once from the SAP fail file. From these, the condition space could be built.
\paragraph{Edges' list generation:}
The edges' list was generated from three sources: 
\begin{itemize}
	\item{electric errors} with all three columns,
	\item{SAP fails once} with all columns except the service task(s) end timestamp,
	\item{SAP fails once more with a twist} all columns except the service task(s) begin timestamp.
\end{itemize}

This dataframe\footnote{Containing three columns} was processed further. The dataframe's rows were grouped by vehicle ID and arranged by the timestamps\footnote{The similar rows with close timestamp were not been eliminated.}.
The directed edges need a starting and ending point. To achieve that purpose, the starting states were derived from the previous row ending state by vehicle ID. After that, the data was saved in distinct files by vehicle ID.
%----------------------------------------------------------------------------
	\subsection{Exploring}
		\paragraph{Data summaries:}
		With this special dataset the common data summary functions are nearly useless, because of the lack of quantitative variables, and the condition state representation.
		The useful data summaries were informative about the length of the dissimilar edges' and nodes' lists, because it helped to confirm the quality of the data for graph representation and RUL calculation. 

%----------------------------------------------------------------------------		
		\paragraph{Visualizations:}
		The visualizations are beneficial when it comes to a graph represented data. In this project I had used the NetworkX built in graph building and visualization, and the matplotlib package plotting functions.

		As is seems, it is difficult to make a guess, but it can be seen the opportunity to statistically using the states and the edges' weight to make a prediction.
			\begin{figure}[H]
			\centering
			\includegraphics[width=150mm, keepaspectratio]{figures/gen_files/fault_pred_files/path.png}
			\caption{Ten forklifts' paths displayed undirected.} 
			\end{figure}
%----------------------------------------------------------------------------
	\subsection{Calculating RUL}
			\paragraph{Built of graph:}
			When the previous ordering, exploring, and fact checking were completed, the graph building could have begun from generated files. Thus, the base is the nodes' list, moreover, all nodes should be represented in the graph. When it came to edges, the situation was different. For computational resource reasons not all of the paths were included for sure. But more the paths the better was the prediction.
			\paragraph{Important Note:}
			The STILL company, due to more important projects and tasks, not provided electric error data and SAP fault data with merging truck ID on time (2017.12.08.). Thus, the following lines are just implementation plans, and the functions couldn't be tested for real by the due date of the thesis.
			\paragraph{Computing steps:}
			When graph has been build, the remaining useful life calculation steps were the following:
			\begin{enumerate}
				\item{getting a warning sequence, what is not represented in the graph from the same machine in a chronological order,}
				\item{listing the machine' IDs between the oldest and the second oldest nodes given in the sequence,}
				\item{iterating through the whole sequence and eliminating the machines which were absent in the current edges,}
				\item{eliminating and redo when there is no machine ID was returned by the sequence,}
				\item{the list of machines with similar path was given from the last and youngest node in the sequence. The shortest similar machine path was listed to the exchange part ID nodes,}
				\item{from the last node the possibility calculated to all exchange part ID nodes, based on the similar shortest machine path count to the specific exchange part ID nodes divided by sum of to all the exchange part ID nodes,}
				\item{besides of possibility, the time to the specific exchange part ID node was computed to give more insights about the remaining useful life based on the similar shortest machine's distance in time,}
				\item{the result were contained following columns: exchange part ID node, minimal, mean, median and maximal time to that node, and the computed possibility arranged by possibility and time,} 
		 		\item{the RUL is the highest possibility with the shortest remaining time\footnote{The RUL can be some kind of meta score based on time window and possibility level.}}
		 	\end{enumerate}
%----------------------------------------------------------------------------
	\subsection{Result}
The calculation of the whole table is profitable, because technical workers can solve more problem at one repair sessions, resulting in longer timespan on duty for the machines.

The whole electrical failure prediction RUL calculation workflow is in the Appendix \textbf{\ref{appendix:FaultPredRULCalc}} .

The code can be viewed on my GitHub repository \cite{GitHub_FP_RUL}.